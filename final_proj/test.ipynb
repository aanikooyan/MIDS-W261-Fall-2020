{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%pyspark\n",
        "from pyspark.sql import functions as f\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, NullType, ShortType, DateType, BooleanType, BinaryType, TimestampType, Row\n",
        "from pyspark.sql import SQLContext\n",
        "from pyspark.sql.window import Window\n",
        "from datetime import datetime, timedelta\n",
        "from math import sin, cos, atan2, radians, sqrt\n",
        "from us import states\n",
        "import numpy as np\n",
        "\n",
        "sqlContext = SQLContext(sc)\n",
        "\n",
        "S3_BUCKET  = \"s3://filetransfers3\"\n",
        "\n",
        "airlines = spark.read.option(\"header\", \"true\").parquet(f\"{S3_BUCKET}/airlines/201*.parquet\")\n",
        "weather = spark.read.option(\"header\", \"true\").parquet(f\"{S3_BUCKET}/weather/*.parquet\")\n",
        "\n",
        "open_airport_read_location = f\"{S3_BUCKET}/openflights_airports.csv\"\n",
        "open_airport_columns = [\"ID\", \"Name\", \"City\" , \"Country\", \"IATA\", \"ICAO\", \n",
        "                        \"Latitude\", \"Longitude\", \"Altitude\", \"Timezone\", \"DST\", \"zone\", \"Type\", \"Source\" ]\n",
        "\n",
        "cleaned_airline_path = f\"{S3_BUCKET}/cleaned/airlines\"\n",
        "cleaned_weather_path = f\"{S3_BUCKET}/cleaned/weather\"\n",
        "closest_station_path = f\"{S3_BUCKET}/cleaned/closest_station_look_up\"\n",
        "airlines_weather_with_dup_path = f\"{S3_BUCKET}/cleaned/airline_weather_dup\"\n",
        "airline_weather_path = f\"{S3_BUCKET}/cleaned/airline_weather_cleaned\"\n",
        "imputed_data_path = f\"{S3_BUCKET}/cleaned/airline_weather_imputed_indicator_added\"\n",
        "# Airlines Data Processing\n",
        "%pyspark\n",
        "\n",
        "# assign a unique index to airlines dataset for tracking and joinig later\n",
        "airlines_idx = airlines.withColumn(\"FLIGHT_IDX\", f.monotonically_increasing_id())\n",
        "\n",
        "# remove the cancelled flights and keep only the columns of potential interest\n",
        "keep_columns = ['FLIGHT_IDX', 'YEAR','QUARTER','MONTH','DAY_OF_MONTH','DAY_OF_WEEK','FL_DATE', 'OP_UNIQUE_CARRIER', 'TAIL_NUM', \n",
        "                'ORIGIN', 'ORIGIN_CITY_NAME', 'ORIGIN_STATE_ABR', 'DEST', 'DEST_CITY_NAME', \n",
        "                'CRS_DEP_TIME', 'DEP_TIME', 'DEP_DELAY', 'DEP_DELAY_NEW','DEP_DEL15', 'DEP_TIME_BLK', 'CRS_ELAPSED_TIME', \n",
        "                'CRS_ARR_TIME', 'ARR_TIME', 'ARR_DELAY', 'ARR_DELAY_NEW', 'ARR_DEL15', 'CARRIER_DELAY','WEATHER_DELAY', 'NAS_DELAY','SECURITY_DELAY','LATE_AIRCRAFT_DELAY']\n",
        "\n",
        "not_cancelled = airlines_idx.filter(\"CANCELLED==0 AND DEP_TIME is not null AND ARR_TIME is not null\").select(*keep_columns)\n",
        "\n",
        "# convert departure and arrival times to timestamp\n",
        "# actual departure/arrival can happen at midnight, noted as 2400, they are converted to 0:00 of the next day\n",
        "def extract_datetimestamp(year,month,day,hhmm):\n",
        "  hour = int(hhmm)//100\n",
        "  minutes = int(hhmm) - (hour * 100)\n",
        "  if hour == 24:\n",
        "      hour = 0\n",
        "  dt = datetime(int(year),int(month),int(day), hour, minutes)\n",
        "  if hour == 24:\n",
        "      dt = dt + timedelta(hours = 24)\n",
        "  return dt\n",
        "\n",
        "extract_datetimestamp_udf = f.udf(extract_datetimestamp, TimestampType())\n",
        "not_cancelled = not_cancelled.withColumn(\"CRS_DEP_LOCAL\", extract_datetimestamp_udf(\"YEAR\",\"MONTH\",\"DAY_OF_MONTH\",\"CRS_DEP_TIME\"))\n",
        "not_cancelled = not_cancelled.withColumn(\"DEP_LOCAL\", extract_datetimestamp_udf(\"YEAR\",\"MONTH\",\"DAY_OF_MONTH\",\"DEP_TIME\"))\n",
        "not_cancelled = not_cancelled.withColumn(\"CRS_ARR_LOCAL\", extract_datetimestamp_udf(\"YEAR\",\"MONTH\",\"DAY_OF_MONTH\",\"CRS_ARR_TIME\"))\n",
        "not_cancelled = not_cancelled.withColumn(\"ARR_LOCAL\", extract_datetimestamp_udf(\"YEAR\",\"MONTH\",\"DAY_OF_MONTH\",\"ARR_TIME\"))\n",
        "\n",
        "# create a new column that adds two hours to each flight's CRS departure time, \n",
        "# when calculating hourly flight rate, this column is used to join back to main flight data\n",
        "# so that the delay info is joined to flights 2 hours later (effectively giving each flight delay data\n",
        "# 2 hours before departure.)\n",
        "delay_add_2h_udf = f.udf(lambda x: x + timedelta(hours = 2), TimestampType())\n",
        "not_cancelled = not_cancelled.withColumn(\"CRS_DEP_PLUS2\", delay_add_2h_udf(\"CRS_DEP_LOCAL\"))\n",
        "\n",
        "# create a string yyyy-mm-dd hh, group by this later to calculate hourly flight delay\n",
        "datetime_hour_udf = f.udf(lambda x: x.strftime(\"%Y-%m-%d %H\"), StringType())\n",
        "not_cancelled = not_cancelled.withColumn(\"CRS_DEP_STR\", datetime_hour_udf(\"CRS_DEP_LOCAL\"))\n",
        "not_cancelled = not_cancelled.withColumn(\"CRS_DEP_PLUS2_STR\", datetime_hour_udf(\"CRS_DEP_PLUS2\"))\n",
        "\n",
        "# rank occurred flights for each tail number in order to joining of previous flight. \n",
        "# Rank is sorting by based on departure and arrival time, CRS and actual, because there are errorenous \n",
        "# data where two flights of the same physical plane have the same departure/arrival time\n",
        "not_cancelled = not_cancelled.withColumn(\"TIME_RANK\", f.dense_rank()\\\n",
        "                .over(Window.partitionBy(\"TAIL_NUM\")\\\n",
        "                .orderBy(f.desc(\"CRS_DEP_LOCAL\"), f.desc(\"DEP_LOCAL\"), \\\n",
        "                 f.desc(\"CRS_ARR_LOCAL\"), f.desc(\"ARR_LOCAL\"))))\n",
        "\n",
        "not_cancelled.cache()\n",
        "\n",
        "# create a previous flight dataframe, reduce the time rank by 1, so when joining by time rank, \n",
        "# the main table's flight is joined to one flight earlier on the previous flight table\n",
        "prev_flights = not_cancelled.selectExpr('FLIGHT_IDX as PREV_FLIGHT_IDX', \n",
        "                                        'TAIL_NUM as PREV_TAIL_NUM',\n",
        "                                        'TIME_RANK as PREV_TIME_RANK', \n",
        "                                        'CRS_DEP_LOCAL as PREV_CRS_DEP_LOCAL', \n",
        "                                        'DEP_LOCAL AS PREV_DEP_LOCAL',\n",
        "                                        'DEP_DELAY as PREV_DEP_DELAY', \n",
        "                                        'DEP_DELAY_NEW as PREV_DEP_DELAY_NEW', \n",
        "                                        'DEP_DEL15 as PREV_DEP_DEL15', \n",
        "                                        'CRS_ELAPSED_TIME as PREV_CRS_ELAPSED_TIME', \n",
        "                                        'CRS_ARR_LOCAL as PREV_CRS_ARR_LOCAL', \n",
        "                                        'ARR_LOCAL as PREV_ARR_LOCAL',\n",
        "                                        'ARR_DELAY as PREV_ARR_DELAY', \n",
        "                                        'ARR_DELAY_NEW as PREV_ARR_DELAY_NEW', \n",
        "                                        'ARR_DEL15 as PREV_ARR_DEL15')\n",
        "\n",
        "prev_flights = prev_flights.withColumn('PREV_TIME_RANK', prev_flights.PREV_TIME_RANK - 1)\n",
        "\n",
        "# join main flight and previous flight on time rank and tail number,\n",
        "# calculate the time from previous flight's actual departure time to the current flight's CRS departure time\n",
        "# because ACTUAL_DEP_DT_PREV - CRS_DEP_DT will inherently be large for longer flight routes and smaller for short commuter flights\n",
        "# also subtract the CRS elapsed time of the previous flight\n",
        "# the result can be conceptually thought of as the time ground crew have to prepare the plane for current flight departure\n",
        "# short ground crew time and negative ground crew time will likely lead to aircraft delays\n",
        "joined_prev = not_cancelled.join(prev_flights, (not_cancelled.TAIL_NUM == prev_flights.PREV_TAIL_NUM) & \\\n",
        "                                 (not_cancelled.TIME_RANK == prev_flights.PREV_TIME_RANK))\n",
        "\n",
        "time_between_udf =  f.udf(lambda x, y: int((x - y).seconds/60), IntegerType())                                \n",
        "joined_prev = joined_prev.withColumn(\"PREV_DEP_TO_CURR_CRSDEP\", time_between_udf('CRS_DEP_LOCAL', 'PREV_DEP_LOCAL'))\n",
        "joined_prev = joined_prev.withColumn('PREP_TIME', joined_prev.PREV_DEP_TO_CURR_CRSDEP - joined_prev.PREV_CRS_ELAPSED_TIME)\n",
        "\n",
        "# number of flight per hour at each airport\n",
        "hourly_flight = not_cancelled.groupby(['ORIGIN', 'CRS_DEP_PLUS2_STR']).count()\n",
        "hourly_flight = hourly_flight.selectExpr('ORIGIN as ORIGIN_MERGE', 'CRS_DEP_PLUS2_STR as HOUR_MERGE', 'count as HOURLY_NUM_OF_FLIGHTS')\n",
        "\n",
        "# number of delayed flights per hour at each airport\n",
        "hourly_delay = not_cancelled.groupby(['ORIGIN', 'CRS_DEP_PLUS2_STR'])\\\n",
        "                            .agg(f.sum(\"DEP_DEL15\").alias(\"HOURLY_NUM_OF_DELAY\"),\\\n",
        "                                 f.avg(\"DEP_DELAY\").alias(\"HOURLY_MINUTE_OF_DELAY\"), \\\n",
        "                                 f.avg(\"DEP_DELAY_NEW\").alias(\"HOURLY_MINUTE_OF_DELAY_NEW\"))\n",
        "\n",
        "hourly_flight_merged = hourly_flight.join(hourly_delay, \\\n",
        "                      (hourly_flight.ORIGIN_MERGE == hourly_delay.ORIGIN) & \n",
        "                      (hourly_flight.HOUR_MERGE == hourly_delay.CRS_DEP_PLUS2_STR))\\\n",
        "                      .drop(hourly_delay.ORIGIN)\\\n",
        "                      .drop(hourly_delay.CRS_DEP_PLUS2_STR)\n",
        "\n",
        "# calculate delay rate\n",
        "hourly_flight_merged = hourly_flight_merged.withColumn(\"HOURLY_DELAY_RATE\",  (f.col(\"HOURLY_NUM_OF_DELAY\") / f.col(\"HOURLY_NUM_OF_FLIGHTS\")))\n",
        "\n",
        "# join delay info to main flight table\n",
        "joined_prev_delay = joined_prev.join(hourly_flight_merged, (joined_prev.ORIGIN == hourly_flight_merged.ORIGIN_MERGE) & \\\n",
        "                                     (joined_prev.CRS_DEP_STR == hourly_flight_merged.HOUR_MERGE), 'leftouter')\n",
        "                                     \n",
        "open_airports = spark.read.format(\"csv\").option(\"header\",\"false\").load(open_airport_read_location).toDF(*open_airport_columns)\n",
        "open_airports = open_airports.select(\"IATA\", \"Latitude\", \"Longitude\", \"Altitude\")\n",
        "\n",
        "airlines_longlat = joined_prev_delay.join(open_airports, joined_prev_delay.ORIGIN == open_airports.IATA)\n",
        "airlines_longlat = airlines_longlat.withColumn(\"AIRPORT_LATITUDE\", airlines_longlat[\"Latitude\"].cast(\"double\")).drop('Latitude')\n",
        "airlines_longlat = airlines_longlat.withColumn(\"AIRPORT_LONGITUDE\", airlines_longlat[\"Longitude\"].cast(\"double\")).drop('Longitude')\n",
        "airlines_longlat = airlines_longlat.withColumn(\"AIRPORT_ALTITUDE\", airlines_longlat[\"Altitude\"].cast(\"double\")).drop('Altitude')\n",
        "\n",
        "# create a timezone dataframe for the main 50 states + Puerto Rico and Virgin Islands\n",
        "distinct_states = [x.ORIGIN_STATE_ABR for x in airlines_longlat.select('ORIGIN_STATE_ABR').distinct().collect()]\n",
        "state_tz_tuple = [(x, states.lookup(x).time_zones[0]) for x in distinct_states if states.lookup(x)]\n",
        "state_schema = ['state', 'state_tz']\n",
        "state_timezone = spark.createDataFrame(data = state_tz_tuple, schema = state_schema)\n",
        "\n",
        "# manually define dataframe for trusted terroritories\n",
        "tt_tuples = [(\"Guam, TT\",  'Pacific/Guam'), \n",
        "             (\"Saipan, TT\",  'Pacific/Saipan'), \n",
        "             (\"Pago Pago, TT\",  'Pacific/Pago_Pago') ]\n",
        "tt_schema = [\"tt_city\",  'tt_tz']\n",
        "tt_timezone = spark.createDataFrame(data=tt_tuples, schema = tt_schema)\n",
        "\n",
        "airlines_tz = airlines_longlat.join(tt_timezone, airlines_longlat.ORIGIN_CITY_NAME == tt_timezone.tt_city, 'leftouter')\n",
        "airlines_tz = airlines_tz.join(state_timezone, airlines_tz.ORIGIN_STATE_ABR == state_timezone.state, 'leftouter')\n",
        "airlines_tz = airlines_tz.withColumn('ORIGIN_TIMEZONE', f.coalesce('tt_tz', 'state_tz'))\n",
        "\n",
        "# convert local departure time to UTC using timezone\n",
        "airlines_tz = airlines_tz.withColumn(\"CRS_DEP_UTC\", f.to_utc_timestamp(f.col(\"CRS_DEP_LOCAL\"), f.col(\"ORIGIN_TIMEZONE\")))\n",
        "\n",
        "keep_columns = ['FLIGHT_IDX', 'YEAR','QUARTER','MONTH','DAY_OF_MONTH','DAY_OF_WEEK','FL_DATE', 'OP_UNIQUE_CARRIER', 'TAIL_NUM', \n",
        "                'ORIGIN', 'ORIGIN_CITY_NAME', 'ORIGIN_STATE_ABR', 'ORIGIN_TIMEZONE', 'DEST', 'DEST_CITY_NAME', \n",
        "                'CRS_DEP_TIME', 'DEP_TIME', 'DEP_DELAY', 'DEP_DELAY_NEW','DEP_DEL15', 'DEP_TIME_BLK', 'CRS_ELAPSED_TIME', \n",
        "                'CRS_ARR_TIME', 'ARR_TIME', 'ARR_DELAY', 'ARR_DELAY_NEW', 'ARR_DEL15', \n",
        "                'CRS_DEP_LOCAL', 'CRS_DEP_UTC', 'PREV_DEP_TO_CURR_CRSDEP', 'PREP_TIME', \n",
        "                'PREV_FLIGHT_IDX', 'PREV_CRS_DEP_LOCAL', 'PREV_DEP_LOCAL', 'PREV_CRS_ELAPSED_TIME', \n",
        "                'PREV_DEP_DELAY', 'PREV_DEP_DELAY_NEW', 'PREV_DEP_DEL15', \n",
        "                'PREV_CRS_ARR_LOCAL', 'PREV_ARR_LOCAL', 'PREV_ARR_DELAY','PREV_ARR_DELAY_NEW', 'PREV_ARR_DEL15',\n",
        "                'HOURLY_NUM_OF_FLIGHTS', 'HOURLY_NUM_OF_DELAY', 'HOURLY_DELAY_RATE', 'HOURLY_MINUTE_OF_DELAY', 'HOURLY_MINUTE_OF_DELAY_NEW',\n",
        "                'AIRPORT_LATITUDE', 'AIRPORT_LONGITUDE', 'AIRPORT_ALTITUDE']\n",
        "\n",
        "# select cleaned columns and save\n",
        "airlines_clean = airlines_tz.select(*keep_columns)\n",
        "airlines_clean.write.parquet(cleaned_airline_path)\n",
        "# Weather Data Processing\n",
        "%pyspark\n",
        "#use udf to filter out only weather data in us\n",
        "def usa_filter(line):\n",
        "  parsed_line = line.split(\",\")\n",
        "  if len(parsed_line) <2: \n",
        "    return False\n",
        "  _, location = parsed_line\n",
        "  location_list = location.strip().split(\" \")\n",
        "  if len(location_list) == 2: #example AK US\n",
        "    country = location_list[1] \n",
        "    if country in [\"US\", \"GQ\", \"AQ\"]:\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "usa_filter_udf = f.udf(usa_filter, BooleanType())\n",
        "us_weather = weather.filter(usa_filter_udf(\"Name\"))\n",
        "\n",
        "# extract state name for weather stations\n",
        "schema = StructType([StructField(\"State\", StringType(), False), StructField(\"StationName\", StringType(), False)])\n",
        "\n",
        "def extract_name_state(line):\n",
        "  name, remaining = line.split(\",\")\n",
        "  state, _ = remaining.strip().split(\" \")\n",
        "  return Row(\"State\", \"StationName\")(state, name)\n",
        "\n",
        "extract_name_state_udf = f.udf(extract_name_state, schema)\n",
        "\n",
        "us_weather = us_weather.withColumn(\"Output\", f.explode(f.array(extract_name_state_udf(us_weather[\"Name\"]))))\n",
        "us_weather = us_weather.select(\"*\", \"Output.*\").drop(us_weather[\"Name\"])\n",
        "\n",
        "# udf will return null values for any readings with bad qualities\n",
        "bad_quality = ['2', '3', '6', '7']\n",
        "\n",
        "# Wind Speed, Wind Angle\n",
        "schema = StructType([StructField(\"WindSpeed\", IntegerType(), False),\n",
        "                     StructField(\"WindAngle\", IntegerType(), False)])\n",
        "\n",
        "def extract_wind(line):\n",
        "    angle, directionQuality, types, speed, speedQuality = line.split(\",\")\n",
        "    if directionQuality in bad_quality:\n",
        "        angle = 999\n",
        "    if speedQuality in bad_quality:\n",
        "        speed = 9999\n",
        "    return Row(\"WindSpeed\", \"WindAngle\")(int(speed), int(angle))\n",
        "\n",
        "extract_wind_udf = f.udf(extract_wind, schema)\n",
        "\n",
        "preprocess_weather = us_weather.withColumn(\"Output\", f.explode(f.array(extract_wind_udf(us_weather[\"WND\"]))))\n",
        "preprocess_weather = preprocess_weather.select(\"*\", \"Output.*\").drop(us_weather[\"WND\"]).drop(\"Output\")\n",
        "\n",
        "# Vertical Visibility\n",
        "def extract_cig(line):\n",
        "  verticalVisibility, quality, determination, cavok = line.split(\",\")\n",
        "  if quality in bad_quality:\n",
        "      verticalVisibility = 99999\n",
        "  return int(verticalVisibility)\n",
        "\n",
        "extract_cig_udf = f.udf(extract_cig, IntegerType())\n",
        "preprocess_weather = preprocess_weather.withColumn(\"VerticalVisibility\", extract_cig_udf(preprocess_weather[\"CIG\"])).drop(\"CIG\")\n",
        "\n",
        "# Visibility\n",
        "def extract_vis(line):\n",
        "  visibility, quality, variability, variability_quality = line.split(\",\")\n",
        "  if quality in bad_quality:\n",
        "      visibility = 99999\n",
        "  return int(visibility)\n",
        "\n",
        "extract_vis_udf = f.udf(extract_vis, IntegerType())\n",
        "\n",
        "preprocess_weather = preprocess_weather.withColumn(\"Visibility\", extract_vis_udf(preprocess_weather[\"VIS\"])).drop(\"VIS\")\n",
        "\n",
        "# Temperature\n",
        "def extract_tmp(line):\n",
        "  tmp, quality = line.split(\",\")\n",
        "  if quality in bad_quality:\n",
        "      tmp = 9999\n",
        "  return float(tmp)/10\n",
        "\n",
        "extract_tmp_udf = f.udf(extract_tmp, DoubleType())\n",
        "\n",
        "preprocess_weather = preprocess_weather.withColumn(\"Temperature\", extract_tmp_udf(preprocess_weather[\"TMP\"])).drop(\"TMP\")\n",
        "\n",
        "# Dew point temperature\n",
        "def extract_dew(line):\n",
        "  dtmp, quality = line.split(\",\")\n",
        "  if quality in bad_quality:\n",
        "      dtmp = 9999\n",
        "  return float(dtmp)/10\n",
        "\n",
        "extract_dew_udf = f.udf(extract_dew, DoubleType())\n",
        "\n",
        "preprocess_weather = preprocess_weather.withColumn(\"DewPointTemp\", extract_dew_udf(preprocess_weather[\"DEW\"])).drop(\"DEW\")\n",
        "\n",
        "# Sea Level Pressure\n",
        "def extract_slp(line):\n",
        "  slp, quality = line.split(\",\")\n",
        "  if quality in bad_quality:\n",
        "      slp = 99999\n",
        "  return int(slp)\n",
        "\n",
        "extract_slp_udf = f.udf(extract_slp, IntegerType())\n",
        "\n",
        "preprocess_weather = preprocess_weather.withColumn(\"SeaLevelPressure\", extract_slp_udf(preprocess_weather[\"SLP\"])).drop(\"SLP\")\n",
        "\n",
        "schema2 = StructType([StructField(\"CloudMode\", IntegerType(), False), StructField(\"CloudHeight\", IntegerType(), False)])\n",
        "\n",
        "# Cloud Height, Cloud Mode\n",
        "def extract_cloud(line):\n",
        "    if line == '':\n",
        "        return Row(\"CloudMode\", \"CloudHeight\")(int(9), int(99999))\n",
        "    else:\n",
        "        code, oktas, cquality, height, dquality, character  = line.split(\",\")\n",
        "        if cquality in bad_quality:\n",
        "            code = 9\n",
        "        if dquality in bad_quality:\n",
        "            height = 99999\n",
        "        return Row(\"CloudMode\", \"CloudHeight\")(int(code), int(height))\n",
        "\n",
        "extract_cloud_udf = f.udf(extract_cloud, schema2)\n",
        "\n",
        "preprocess_weather = preprocess_weather.withColumn(\"Output\", f.explode(f.array(extract_cloud_udf(preprocess_weather[\"GD1\"]))))\n",
        "preprocess_weather = preprocess_weather.select(\"*\", \"Output.*\").drop(us_weather[\"GD1\"]).drop(\"Output\")\n",
        "\n",
        "# Weather Condition\n",
        "def extract_aw(line):\n",
        "    if line == '':\n",
        "        return int(9999)\n",
        "    code, quality = line.split(\",\")\n",
        "    if quality in bad_quality:\n",
        "        code = 9999\n",
        "    return int(code)\n",
        "\n",
        "extract_aw_udf = f.udf(extract_aw, IntegerType())\n",
        "\n",
        "preprocess_weather = preprocess_weather.withColumn('AW', f.coalesce('AW1', 'AW2', 'AW3', 'AW4', 'AW5', 'AW6', 'AW7')).drop('AW1', 'AW2', 'AW3', 'AW4', 'AW5', 'AW6', 'AW7')\n",
        "preprocess_weather = preprocess_weather.withColumn(\"AtmCondition\", extract_aw_udf(preprocess_weather[\"AW\"])).drop(\"AW\")\n",
        "\n",
        "# remove the null value placeholders\n",
        "def find_null(value, null_placeholder):\n",
        "    return f.when(value != null_placeholder, value).otherwise(f.lit(None))\n",
        "\n",
        "null_pairs = [('WindAngle', 999), ('WindSpeed', 9999), ('VerticalVisibility', 99999),\n",
        "              ('Visibility', 999999), ('Temperature', 999.9), ('DewPointTemp', 999.9),\n",
        "              ('SeaLevelPressure', 99999), ('CloudMode', 9), ('CloudHeight', 99999),\n",
        "              ('AtmCondition', 9999)]\n",
        "\n",
        "for pair in null_pairs:\n",
        "    col_name, null_value = pair\n",
        "    preprocess_weather = preprocess_weather.withColumn(col_name, find_null(f.col(col_name), null_value))\n",
        "\n",
        "# save\n",
        "weather_clean = preprocess_weather.selectExpr([\"STATION as WeatherStationID\", \"StationName as WeatherStationName\", \n",
        "    \"LATITUDE as WeatherStationLatitude\", \"LONGITUDE as WeatherStationLongitude\", \"DATE as WeatherTimestamp\", \n",
        "    'WindAngle', 'WindSpeed', 'VerticalVisibility', 'Visibility', 'Temperature', \n",
        "    'DewPointTemp','SeaLevelPressure', 'CloudMode', 'CloudHeight', 'AtmCondition'])\n",
        "\n",
        "weather_clean.write.parquet(cleaned_weather_path)\n",
        "# Joining Airlines and Weather\n",
        "%pyspark\n",
        "\n",
        "airlines_clean = spark.read.option(\"header\", \"true\").parquet(cleaned_airline_path)\n",
        "weather_clean = spark.read.option(\"header\", \"true\").parquet(cleaned_weather_path)\n",
        "open_airports = spark.read.format(\"csv\").option(\"header\",\"false\").load(open_airport_read_location).toDF(*open_airport_columns)\n",
        "\n",
        "# get distinct airports in the airlines dataset, join with iata table for long/lat\n",
        "unique_iata_code = airlines_clean.select('ORIGIN').distinct()\n",
        "unique_iata_location = unique_iata_code.join(open_airports, unique_iata_code.ORIGIN == open_airports.IATA)\\\n",
        "                      .select(\"IATA\", \"Latitude\", \"Longitude\", \"Altitude\")\n",
        "\n",
        "# For every airport, find the closest weather station.\n",
        "us_weather_station = weather_clean.select(\"WeatherStationID\", \"WeatherStationLatitude\", \"WeatherStationLongitude\").distinct()\n",
        "\n",
        "possible_pairs = unique_iata_location.join(us_weather_station)\n",
        "possible_pairs_rdd = possible_pairs.rdd\n",
        "\n",
        "# compute distance between each pair of airport and weather station\n",
        "def compute_distance(line):\n",
        "  code, lat1, lon1, lat, station, lat2, lon2 = line\n",
        "  lat1 = float(lat1)\n",
        "  lon1 = float(lon1)\n",
        "  lat2 = float(lat2)\n",
        "  lon2 = float(lon2)\n",
        "  \n",
        "  radius = 6371 # km\n",
        "  dlat = radians(lat2-lat1)\n",
        "  dlon = radians(lon2-lon1)\n",
        "  a = sin(dlat/2) * sin(dlat/2) + cos(radians(lat1)) \\\n",
        "      * cos(radians(lat2)) * sin(dlon/2) * sin(dlon/2)\n",
        "  c = 2 * atan2(sqrt(a), sqrt(1-a))\n",
        "  d = radius * c\n",
        "  return code, station, d\n",
        "\n",
        "def get_min(a,b):\n",
        "  if a[1] < b[1]:\n",
        "    return a\n",
        "  else:\n",
        "    return b\n",
        "\n",
        "pairs_rdd_with_distance = possible_pairs_rdd.map(compute_distance).cache()\n",
        "\n",
        "# select the 3 closest weather station for each airport\n",
        "iata_three_closest_stations = pairs_rdd_with_distance.toDF([\"IATA\", \"WeatherStationID\", \"Distance\"])\\\n",
        "    .select(\"*\", f.row_number()\\\n",
        "    .over(Window.partitionBy(\"IATA\").orderBy(f.col(\"Distance\"))).alias(\"rowNum\"))\\\n",
        "    .where(f.col(\"rowNum\") <= 3).drop(\"rowNum\")\n",
        "\n",
        "# remove the station if more than threshold distance away\n",
        "threshold_km = 30\n",
        "closest_stations = iata_three_closest_stations.filter(f\"Distance < {threshold_km}\")\n",
        "\n",
        "closest_stations.write.parquet(closest_station_path)%pyspark\n",
        "\n",
        "airlines_clean = spark.read.option(\"header\", \"true\").parquet(cleaned_airline_path)\n",
        "weather_clean = spark.read.option(\"header\", \"true\").parquet(cleaned_weather_path)\n",
        "closest_stations = spark.read.option(\"header\", \"true\").parquet(closest_station_path)\n",
        "\n",
        "airlines_join = airlines_clean.join(closest_stations, \n",
        "                                 airlines_clean.ORIGIN == closest_stations.IATA, \"inner\")\\\n",
        "                                .select(['FLIGHT_IDX', 'WeatherStationID', 'CRS_DEP_UTC'])\n",
        "\n",
        "# add time range for join (2 ~ 3 hours prior departure time)\n",
        "start_datetime_udf = f.udf(lambda x: x + timedelta(hours = -3), TimestampType())\n",
        "end_datetime_udf = f.udf(lambda x: x + timedelta(hours = -2), TimestampType())\n",
        "airlines_join = airlines_join.withColumn(\"WEATHER_START\", start_datetime_udf(\"CRS_DEP_UTC\"))\n",
        "airlines_join = airlines_join.withColumn(\"WEATHER_END\", end_datetime_udf(\"CRS_DEP_UTC\"))\n",
        "\n",
        "# join airlines and weather\n",
        "airlines_weather = airlines_join.join(weather_clean, (weather_clean.WeatherStationID == airlines_join.WeatherStationID) \\\n",
        "                                      & (weather_clean.WeatherTimestamp > airlines_join.WEATHER_START) \\\n",
        "                                      & (weather_clean.WeatherTimestamp < airlines_join.WEATHER_END ))\\\n",
        "                                      .drop(weather_clean.WeatherStationID)\n",
        "\n",
        "airlines_weather.write.parquet(airlines_weather_with_dup_path)\n",
        "# Aggregate multiple weather readings per flight\n",
        "%pyspark\n",
        "airlines_weather = spark.read.option(\"header\", \"true\").parquet(airlines_weather_with_dup_path)\n",
        "airlines_clean = spark.read.option(\"header\", \"true\").parquet(cleaned_airline_path)\n",
        "\n",
        "airlines_weather_numeric = airlines_weather\\\n",
        "      .select(['FLIGHT_IDX', 'WindAngle', 'WindSpeed', 'VerticalVisibility', 'Visibility', \n",
        "      'Temperature', 'DewPointTemp', 'SeaLevelPressure', 'CloudHeight']).groupby('FLIGHT_IDX').mean()\n",
        "\n",
        "# numeric values\n",
        "airlines_weather_numeric = airlines_weather_numeric\\\n",
        "      .selectExpr('FLIGHT_IDX', \n",
        "                  '`avg(WindAngle)` as WindAngle', \n",
        "                  '`avg(WindSpeed)` as WindSpeed', \n",
        "                  '`avg(VerticalVisibility)` as VerticalVisibility', \n",
        "                  '`avg(Visibility)` as Visibility', \n",
        "                  '`avg(Temperature)` as Temperature', \n",
        "                  '`avg(DewPointTemp)` as DewPointTemp', \n",
        "                  '`avg(SeaLevelPressure)` as SeaLevelPressure', \n",
        "                  '`avg(CloudHeight)` as CloudHeight')\n",
        "\n",
        "# categorical values\n",
        "CloudMode_df = airlines_weather.select(['FLIGHT_IDX', 'CloudMode']).where(f.col(\"CloudMode\").isNotNull())\n",
        "grouped1 = CloudMode_df.groupBy('FLIGHT_IDX', 'CloudMode').count()\n",
        "window = Window.partitionBy(\"FLIGHT_IDX\").orderBy(f.desc(\"count\"))\n",
        "CloudMode_mode = grouped1.withColumn('order', f.row_number().over(window)).where(f.col('order') == 1)\n",
        "CloudMode_mode = CloudMode_mode.select('FLIGHT_IDX', 'CloudMode')\n",
        "\n",
        "AtmCondition_df = airlines_weather.select(['FLIGHT_IDX', 'AtmCondition']).where(f.col(\"AtmCondition\").isNotNull())\n",
        "grouped2 = AtmCondition_df.groupBy('FLIGHT_IDX', 'AtmCondition').count()\n",
        "AtmCondition_mode = grouped2.withColumn('order', f.row_number().over(window)).where(f.col('order') == 1)\n",
        "AtmCondition_mode = AtmCondition_mode.select('FLIGHT_IDX', 'AtmCondition')\n",
        "\n",
        "airline_weather_no_dup = airlines_clean.join(airlines_weather_numeric, \\\n",
        "     airlines_clean.FLIGHT_IDX == airlines_weather_numeric.FLIGHT_IDX, 'leftouter')\\\n",
        "    .drop(airlines_weather_numeric.FLIGHT_IDX)\n",
        "\n",
        "airline_weather_no_dup = airline_weather_no_dup.join(CloudMode_mode, \\\n",
        "     airline_weather_no_dup.FLIGHT_IDX == CloudMode_mode.FLIGHT_IDX, 'leftouter')\\\n",
        "    .drop(CloudMode_mode.FLIGHT_IDX)\n",
        "\n",
        "airline_weather_no_dup = airline_weather_no_dup.join(AtmCondition_mode, \\\n",
        "     airline_weather_no_dup.FLIGHT_IDX == AtmCondition_mode.FLIGHT_IDX, 'leftouter')\\\n",
        "    .drop(AtmCondition_mode.FLIGHT_IDX)\n",
        "\n",
        "keep_columns = ['FLIGHT_IDX', 'QUARTER', 'MONTH', 'DAY_OF_WEEK', 'FL_DATE', 'CRS_DEP_TIME', 'DEP_TIME_BLK', \n",
        "                'OP_UNIQUE_CARRIER', 'TAIL_NUM', 'ORIGIN', 'ORIGIN_CITY_NAME', 'ORIGIN_STATE_ABR', 'ORIGIN_TIMEZONE',\n",
        "                'AIRPORT_LATITUDE', 'AIRPORT_LONGITUDE', 'AIRPORT_ALTITUDE', 'DEST', 'DEST_CITY_NAME', 'CRS_ELAPSED_TIME', \n",
        "                'DEP_DELAY', 'DEP_DELAY_NEW', 'DEP_DEL15', 'CRS_DEP_LOCAL', 'CRS_DEP_UTC',\n",
        "                'PREV_DEP_TO_CURR_CRSDEP', 'PREP_TIME', 'HOURLY_DELAY_RATE', 'HOURLY_NUM_OF_FLIGHTS', 'HOURLY_NUM_OF_DELAY',\n",
        "                'PREV_FLIGHT_IDX', 'PREV_CRS_DEP_LOCAL', 'PREV_DEP_LOCAL', 'PREV_DEP_DELAY', 'PREV_DEP_DELAY_NEW', 'PREV_DEP_DEL15', 'PREV_CRS_ELAPSED_TIME',\n",
        "                'PREV_CRS_ARR_LOCAL', 'PREV_ARR_LOCAL', 'PREV_ARR_DELAY', 'PREV_ARR_DELAY_NEW', 'PREV_ARR_DEL15', \n",
        "                'WindAngle', 'WindSpeed', 'VerticalVisibility', 'Visibility', 'Temperature', 'DewPointTemp', 'SeaLevelPressure', 'CloudHeight', 'CloudMode', 'AtmCondition']\n",
        "\n",
        "cleaned_data = airline_weather_no_dup.select(*keep_columns)\n",
        "cleaned_data.write.parquet(airline_weather_path)\n",
        "# Impute missing weather data\n",
        "%pyspark\n",
        "cleaned_data = spark.read.option(\"header\", \"true\").parquet(airline_weather_path)\n",
        "\n",
        "# since weather data is 2-3 hours before CRS_DEP_LOCAL, \n",
        "# subtract 2.5 hours as rough timestmap for weather data\n",
        "def timestamp_to_block(dt):\n",
        "  weather_timestamp = dt + timedelta(hours = -2.5)\n",
        "  hour = int(weather_timestamp.strftime('%H'))\n",
        "  if (hour >= 0) & (hour < 6):\n",
        "      hour_block = 'early'\n",
        "  elif (hour >= 6) & (hour < 12):\n",
        "      hour_block = 'morning'\n",
        "  elif (hour >= 12) & (hour < 18):\n",
        "      hour_block = 'afternoon'\n",
        "  elif (hour >= 18) & (hour < 24):\n",
        "      hour_block = 'night'\n",
        "  return f'{weather_timestamp.strftime(\"%Y-%m-%d\")} {hour_block}'\n",
        "\n",
        "datetime_block_udf = f.udf(timestamp_to_block, StringType())\n",
        "\n",
        "cleaned_data = cleaned_data.withColumn(\"Weather_Timeblock\", datetime_block_udf(\"CRS_DEP_UTC\"))\n",
        "\n",
        "# impute\n",
        "def median(values_list):\n",
        "    med = np.median(values_list)\n",
        "    return float(med)\n",
        "\n",
        "udf_median = f.udf(median, DoubleType())\n",
        "\n",
        "df_medians = cleaned_data.groupBy('ORIGIN','Weather_Timeblock').\\\n",
        "            agg(udf_median(f.collect_list(f.col('WindSpeed'))).alias('WindSpeed_md'), \\\n",
        "                udf_median(f.collect_list(f.col('VerticalVisibility'))).alias('VerticalVisibility_md'), \\\n",
        "                udf_median(f.collect_list(f.col('Visibility'))).alias('Visibility_md'), \\\n",
        "                udf_median(f.collect_list(f.col('Temperature'))).alias('Temperature_md'), \\\n",
        "                udf_median(f.collect_list(f.col('DewPointTemp'))).alias('DewPointTemp_md'),\\\n",
        "                udf_median(f.collect_list(f.col('CloudHeight'))).alias('CloudHeight_md'))\n",
        "                \n",
        "impute_data = cleaned_data.join(df_medians, (cleaned_data.ORIGIN == df_medians.ORIGIN) & (cleaned_data.Weather_Timeblock == df_medians.Weather_Timeblock), 'leftouter').drop(df_medians.ORIGIN)\n",
        "\n",
        "impute_data = impute_data.withColumn('WindSpeed_imp', f.coalesce('WindSpeed', 'WindSpeed_md'))\n",
        "impute_data = impute_data.withColumn('VerticalVisibility_imp', f.coalesce('VerticalVisibility', 'VerticalVisibility_md'))\n",
        "impute_data = impute_data.withColumn('Visibility_imp', f.coalesce('Visibility', 'Visibility_md'))\n",
        "impute_data = impute_data.withColumn('Temperature_imp', f.coalesce('Temperature', 'Temperature_md'))\n",
        "impute_data = impute_data.withColumn('DewPointTemp_imp', f.coalesce('DewPointTemp', 'DewPointTemp_md'))\n",
        "impute_data = impute_data.withColumn('CloudHeight_imp', f.coalesce('CloudHeight', 'CloudHeight_md'))\n",
        "\n",
        "# new indicator columns from EDA\n",
        "def cloudmode_binarize(cm):\n",
        "    if cm == np.nan:\n",
        "        return 0\n",
        "    else:\n",
        "        if cm == 0:\n",
        "            return 1\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "cloudmode_binarize_udf = f.udf(cloudmode_binarize, IntegerType())\n",
        "impute_data = impute_data.withColumn('CloudMode_binary', cloudmode_binarize_udf('CloudMode'))\n",
        "\n",
        "timeblock6_udf = f.udf(lambda x: 1 if x == '0600-0659' else 0, IntegerType())\n",
        "timeblock7_udf = f.udf(lambda x: 1 if x == '0700-0759' else 0, IntegerType())\n",
        "timeblock0_udf = f.udf(lambda x: 1 if x == '0001-0559' else 0, IntegerType())\n",
        "timeblock8_udf = f.udf(lambda x: 1 if x == '0800-0859' else 0, IntegerType())\n",
        "impute_data = impute_data.withColumn(\"BLOCK6\", timeblock6_udf(\"DEP_TIME_BLK\"))\n",
        "impute_data = impute_data.withColumn(\"BLOCK7\", timeblock7_udf(\"DEP_TIME_BLK\"))\n",
        "impute_data = impute_data.withColumn(\"BLOCK0\", timeblock0_udf(\"DEP_TIME_BLK\"))\n",
        "impute_data = impute_data.withColumn(\"BLOCK8\", timeblock8_udf(\"DEP_TIME_BLK\"))\n",
        "\n",
        "impute_data = impute_data.select('FLIGHT_IDX', 'QUARTER', 'MONTH', 'DAY_OF_WEEK', 'FL_DATE', 'CRS_DEP_TIME', 'DEP_TIME_BLK', 'CRS_DEP_UTC', 'OP_UNIQUE_CARRIER', 'TAIL_NUM', 'ORIGIN', 'ORIGIN_CITY_NAME', 'ORIGIN_TIMEZONE', 'AIRPORT_LATITUDE', 'AIRPORT_LONGITUDE', 'AIRPORT_ALTITUDE', 'DEST', 'DEST_CITY_NAME', 'CRS_ELAPSED_TIME', 'DEP_DELAY', 'DEP_DELAY_NEW', 'DEP_DEL15', 'PREV_DEP_TO_CURR_CRSDEP', 'PREP_TIME', 'HOURLY_DELAY_RATE', 'HOURLY_NUM_OF_FLIGHTS', 'HOURLY_NUM_OF_DELAY', 'PREV_FLIGHT_IDX', 'PREV_CRS_DEP_LOCAL', 'PREV_DEP_LOCAL', 'PREV_DEP_DELAY', 'PREV_DEP_DELAY_NEW', 'PREV_DEP_DEL15', 'PREV_CRS_ELAPSED_TIME', 'PREV_CRS_ARR_LOCAL', 'PREV_ARR_LOCAL', 'PREV_ARR_DELAY', 'PREV_ARR_DELAY_NEW', 'PREV_ARR_DEL15', 'WindAngle', 'WindSpeed_imp', 'VerticalVisibility_imp', 'Visibility_imp', 'Temperature_imp', 'DewPointTemp_imp', 'SeaLevelPressure', 'CloudHeight_imp', 'CloudMode', 'AtmCondition', 'CloudMode_binary', 'BLOCK6', 'BLOCK7', 'BLOCK0', 'BLOCK8')\n",
        "\n",
        "delay_cols = ['HOURLY_DELAY_RATE', 'HOURLY_NUM_OF_FLIGHTS']\n",
        "impute_data = impute_data.na.fill(0, subset = delay_cols)\n",
        "\n",
        "drop_na_cols = ['ORIGIN', 'FL_DATE', 'MONTH', 'DAY_OF_WEEK', 'DEP_TIME_BLK', 'AIRPORT_ALTITUDE', 'PREP_TIME', 'HOURLY_DELAY_RATE', 'HOURLY_NUM_OF_FLIGHTS', 'PREV_DEP_DEL15', 'PREV_CRS_ELAPSED_TIME', 'PREV_ARR_DEL15', 'WindSpeed_imp', 'VerticalVisibility_imp', 'Visibility_imp', 'Temperature_imp', 'DewPointTemp_imp', 'DEP_DELAY_NEW', 'DEP_DEL15']\n",
        "\n",
        "impute_final = impute_data.na.drop(how = 'any', subset = drop_na_cols)\n",
        "impute_final = impute_final\\\n",
        "    .filter('(PREV_CRS_ELAPSED_TIME > 18) AND (CRS_ELAPSED_TIME > 18)')\\\n",
        "    .filter('(PREV_DEP_DELAY > -45) AND (PREV_DEP_DELAY < 1609)')\\\n",
        "    .filter('(PREV_ARR_DELAY > -81) AND (PREV_ARR_DELAY < 1605)')\\\n",
        "    .filter('(DEP_DELAY > -45) AND (DEP_DELAY < 1609)')\n",
        "\n",
        "impute_final.write.parquet(imputed_data_path)%pyspark\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}